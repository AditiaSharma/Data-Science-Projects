{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d026a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries to import\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "from pickle import dump\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210e7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# function to extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the VGG16 model\n",
    "    model = Sequential()\n",
    "    vgg_16 = VGG16()\n",
    "\n",
    "    # Print the model summary to understand the model\n",
    "    print(vgg_16.summary())\n",
    "\n",
    "    # re-structure the model by removing the last layer\n",
    "    #####\n",
    "\n",
    "    for layer in vgg_16.layers[:-1]: \n",
    "        model.add(layer)\n",
    "\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "\n",
    "    # extract features from each photo in a dictionary\n",
    "    features = dict()\n",
    "\n",
    "    # iterate over all the files of the dataset directory\n",
    "    for name in listdir(directory):\n",
    "\n",
    "        filename = directory + '/' + name\n",
    "        # load the image from filename with target size of (224,224)\n",
    "        image = load_img(filename,  target_size=(224, 224))\n",
    "\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        # reshape data for the model (1, 3, 224, 224)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "        # preprocess the image for the VGG model\t\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        # get features (output of the model for the image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "\n",
    "        # store feature in the features dictionary with image_id as key and feature as value\n",
    "        features[image_id] = feature\n",
    "\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = 'Flicker8k_Dataset/'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc88568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:  1000268201_693b08cb0e value:  [[2.5076463 0.        0.        ... 0.        0.        0.       ]]\n",
      "key:  1001773457_577c3a7d70 value:  [[0.        0.        0.4941088 ... 0.        0.        0.       ]]\n",
      "key:  1002674143_1b742ab4b8 value:  [[1.4937083  0.         0.53568405 ... 2.3154142  3.7418432  0.        ]]\n",
      "key:  1003163366_44323f5815 value:  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "key:  1007129816_e794419615 value:  [[0.         0.09227604 0.         ... 0.         0.         0.06529218]]\n",
      "key:  1007320043_627395c3d8 value:  [[0.        0.        0.        ... 0.        3.3386385 0.       ]]\n",
      "key:  1009434119_febe49276a value:  [[2.0962937  2.1193173  3.5624375  ... 0.64264333 2.714653   0.        ]]\n",
      "key:  1012212859_01547e3f17 value:  [[0.        0.        0.9873686 ... 0.        1.4932499 0.8612866]]\n",
      "key:  1015118661_980735411b value:  [[1.4410408 0.2829206 0.        ... 0.        2.4591966 0.       ]]\n",
      "(1, 4096)\n",
      "8091\n"
     ]
    }
   ],
   "source": [
    "#this is the output to understand the feature dictionary variable.\n",
    "\n",
    "i = 0\n",
    "for key, value in features.items():\n",
    "    print(\"key: \", key, \"value: \", value)\n",
    "    i += 1\n",
    "    if i==9:\n",
    "        break\n",
    "\n",
    "print(value.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e086e6",
   "metadata": {},
   "source": [
    "# Preparing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7835e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#load doc into memory\n",
    "\n",
    "def load_doc(filname):\n",
    "    file = open(filname, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dadef6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc(\"Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fff21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = doc.split(\"\\n\")[0].split(\"\\t\")[0].split(\".\")[0]\n",
    "descript = doc.split(\"\\n\")[0].split(\"\\t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8984691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e\n",
      "A child in a pink dress is climbing up a set of stairs in an entry way .\n"
     ]
    }
   ],
   "source": [
    "print(filename)\n",
    "print(descript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed91e60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n",
      "A child in a pink dress is climbing up a set of stairs in an entry way .\n"
     ]
    }
   ],
   "source": [
    "line1 = doc.split('\\n')[0]\n",
    "line1 = line1.split()\n",
    "img_id = line1[0]\n",
    "img_desc = line1[1:]\n",
    "img_desc = ' '.join(img_desc)\n",
    "\n",
    "print(img_id[:-2])\n",
    "print(img_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892c72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0][:-2], tokens[1:]\n",
    "\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860575cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean the text in the following ways in order to reduce the size of the vocabulary of words we will need to work with:\n",
    "\n",
    "# 1. Convert all words to lowercase.\n",
    "# 2. Remove all punctuation.\n",
    "# 3. Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "# 4. Remove all words with numbers in them.\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "\n",
    "            # tokenize desc using split \n",
    "            desc = desc.split()\n",
    "\n",
    "            # convert all the words in desc to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "\n",
    "            # remove punctuation from each token using translate function \n",
    "            # pass the \"table\" variable to this function\n",
    "            desc = [word.translate(table) for word in desc ]\n",
    "\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "\n",
    "            # remove tokens with numbers in them using isalpha\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504bf0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0af9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7d2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# Use the above functions in the code below-\n",
    "\n",
    "filename = 'Flickr8k.token.txt'\n",
    "\n",
    "# load descriptions (pass the filename)\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# parse descriptions (pass doc)\n",
    "descriptions = load_descriptions(doc) \n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "\n",
    "# clean the descriptions\n",
    "#####\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "# summarize descriptions to a vocabulary of words\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions2.txt')\n",
    "\n",
    "# You should get the following output for the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c76408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e6b402",
   "metadata": {},
   "source": [
    "# load deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d36fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load, dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "902bfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "    # fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c68bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE CODE\n",
    "# X1, X2 (text sequence), y (word)\n",
    "# photo\t    startseq, little\n",
    "# photo\t    startseq  little, girl\n",
    "# photo\t    startseq  little  girl, running\n",
    "# photo\t    startseq  little  girl  running, in\n",
    "# photo\t    startseq  little  girl  running  in, field\n",
    "# photo\t    startseq, little, girl, running, in, field,     endseq\n",
    "\n",
    "# You need to create the sequences in abovee format for your data\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ab32d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34, 256)      1940224     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7579)         1947803     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "6000/6000 [==============================] - 3228s 536ms/step - loss: 4.6627\n",
      "6000/6000 [==============================] - 2614s 436ms/step - loss: 3.8977\n",
      "6000/6000 [==============================] - 2692s 449ms/step - loss: 3.6370\n",
      "6000/6000 [==============================] - 2870s 478ms/step - loss: 3.4756\n",
      "6000/6000 [==============================] - 2864s 477ms/step - loss: 3.3788\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    # define an input of shape (4096,)\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    # Add a dropout layer of 0.5\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    # Add a dense layer of 256 units with relu activation\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # sequence model\n",
    "    # Define input of shape (max_length,)\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    # Define an 256 dimension embedding layer with the vocab_size and parameter mark_zero=True\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    # Add a dropout layer of 0.5\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    # Add LSTM Layer\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model\n",
    "    # Add the outputs of feature extractor model and sequence model\n",
    "    decoder1 = Add()([fe2, se3])\n",
    "    # Feed it to a dense layer of 256 units with relu activation\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    # Add a dense layer with no. of units=vocab_size, and softmax activation\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    # tie it together and create a model instance which takes input as [image, seq] and gives output as [word]  \n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "    # compile model with categorical crossentropy loss and adam optimizer\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "\n",
    "    # check if this is same as image provided \n",
    "    try:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    except:\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            # create the sequences\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# load clean descriptions for train\n",
    "train_descriptions = load_clean_descriptions('descriptions2.txt',train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "\n",
    "# load photo features for train using pkl file\n",
    "train_features = load_photo_features('features2.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "# prepare tokenizer for train descriptions\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length of train description\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 5\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch and verbose 1\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3880ae",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13edb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.models import load_model\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence using texts_to_sequences (consider first element)\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad sequence for maxlength\n",
    "        sequence =  pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word using both photo and sequence\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat =  argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    # assign empty lists\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# prepare tokenizer on train set\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# load clean descriptions for train\n",
    "train_descriptions = load_clean_descriptions('descriptions2.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer for train descriptions\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length =  max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# prepare test set\n",
    "\n",
    "# load test set\n",
    "filename = 'Flickr_8k.trainImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions2.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features2.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'model_4.h5'\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58213f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c54c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# extract features from each image in the directory\n",
    "def extract_features(filename):\n",
    "    model = Sequential()\n",
    "    #load model\n",
    "    vgg_16 = VGG16()\n",
    "    # re-structure the model by removing the last layer\n",
    "    for layer in vgg_16.layers[:-1]: # this is where I changed your code\n",
    "        model.add(layer)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('model_4.h5')\n",
    "# load and prepare your own image to generate the caption\n",
    "photo = extract_features('boyinstreet.jpeg')   \n",
    "# generate description\n",
    "description =  generate_desc(model, tokenizer, photo, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163e1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3144f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " man in black shirt and jeans is walking down the street \n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(description[8:-6])\n",
    "# print(description)\n",
    "# provide your image file name \n",
    "Image(filename='boyinstreet.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a5f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103003c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
